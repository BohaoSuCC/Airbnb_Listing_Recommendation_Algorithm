---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: DeskB's Group Project
execute:
  echo: false
  eval: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import requests
# get the current directory
current_dir = os.path.dirname(__file__)

# Set the Github PERMALINK URL for downloading bio.bib and harvard-cite-them-right.csl
# Automatically download the BibTeX file.

bib_url = "https://raw.githubusercontent.com/BohaoSuCC/Groupwork_DeskB/main/bio.bib"

# create local path for saving
local_bib_path = os.path.join(current_dir, "bio.bib")

# download and save .bib
response = requests.get(bib_url)
with open(local_bib_path, 'wb') as file:
    file.write(response.content)

csl_url = "https://raw.githubusercontent.com/jreades/fsds/master/assessments/harvard-cite-them-right.csl"

# create local path for saving
local_csl_path = os.path.join(current_dir, "harvard-cite-them-right.csl")

# download and save .csl 
response = requests.get(csl_url)
with open(local_csl_path, 'wb') as file:
    file.write(response.content)

```

## Declaration of Authorship {.unnumbered .unlisted}

We, \[DeskB\], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 19th December 2023

Student Numbers: 20017359 23032922 23081403 23103585 23130397

## Brief Group Reflection

| What Went Well | What Was Challenging |
|----------------|----------------------|
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

Frankly, we've encountered lots of confusion towards the topic of this assessment.

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```
{{< pagebreak >}}

# Response to Questions

```{python}
import os
import spacy
import pandas as pd
import numpy as np
import geopandas as gpd
import re
import math
import string
import unicodedata
import gensim
import matplotlib as mpl
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib.gridspec import GridSpec
from mpl_toolkits.axes_grid1.inset_locator import inset_axes
import matplotlib.patheffects as PathEffects
import nltk
import seaborn as sns
import ast  # 用于安全地将字符串转换为列表
import umap
import zipfile
import requests
from PIL import Image

import contextily as ctx
import urllib.request

from PIL import ImageDraw

from scipy.spatial import cKDTree
from scipy.spatial.distance import cdist
from scipy.ndimage import convolve
from shapely.geometry import Point

from sklearn.preprocessing import OneHotEncoder  # We don't use this but I point out where you *could*
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.manifold import TSNE
from scipy.cluster.hierarchy import dendrogram, linkage

from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk import ngrams, FreqDist

from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora.dictionary import Dictionary
from gensim.matutils import Sparse2Corpus
from gensim.matutils import corpus2dense
from gensim.models import tfidfmodel
from gensim.models import Word2Vec
from gensim.models import TfidfModel
from gensim.models import KeyedVectors
from gensim.models.ldamodel import LdaModel

from joblib import dump
from joblib import load

from bs4 import BeautifulSoup
from wordcloud import WordCloud, STOPWORDS

# Import everthing from textual/__init__.py
# Including bunch of tools and functions we could use for NLP 
from textual import *
```

```{python}

# check the "Data" folder
data_dir = os.path.join(current_dir, "Data")

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

# check the "Model" folder
model_dir = os.path.join(current_dir, "Model")
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

# check the "Images" folder
iamges_dir = os.path.join(current_dir, "Images")
if not os.path.exists(iamges_dir):
    os.makedirs(iamges_dir)
```

```{python}
# Download and read the csv file remotely from url
host = 'http://data.insideairbnb.com'
path = 'united-kingdom/england/london/2023-09-06/data'
file = 'listings.csv.gz'
url  = f'{host}/{path}/{file}'

# Save csv file
if os.path.exists(file):
  Airbnb_Listing = pd.read_csv(file, compression='gzip', low_memory=False)
else: 
  Airbnb_Listing = pd.read_csv(url, compression='gzip', low_memory=False)
  Airbnb_Listing.to_csv(os.path.join("Data","listing.csv"))
```
```{python}
# Download and read the gpkg file remotely from url
host = 'https://data.london.gov.uk'
path = 'download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f'
file = 'London_Boroughs.gpkg'
url  = f'{host}/{path}/{file}'

# Save gkpg file
if os.path.exists(file):
  London_boroughs = gpd.read_file(file, low_memory=False)
else: 
  London_boroughs = gpd.read_file(url, low_memory=False)
  London_boroughs.to_file(os.path.join("Data","London_Boroughs.gpkg"), driver='GPKG')
```
```{python}

data_dir = os.path.join(current_dir, "Data")

zip_url = "https://data.london.gov.uk/download/statistical-gis-boundary-files-london/08d31995-dd27-423c-a987-57fe8e952990/London-wards-2018.zip"

local_zip_path = os.path.join(data_dir, "London-wards-2018.zip")


response = requests.get(zip_url)
with open(local_zip_path, 'wb') as file:
    file.write(response.content)

with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:
    zip_ref.extractall(data_dir)

London_wards = gpd.read_file(os.path.join("Data","London-wards-2018_ESRI","London_Ward.shp"))

```

## 1. Who collected the data? 

The dataset was collected by Murray Cox through automatic website scraping, specifically for the Inside Airbnb project. Murray Cox utilized automated tools to extract publicly available data from the Airbnb website, including location information and pricing，and then compile it into the dataset.

## 2. Why did they collect it? 

Murray Cox collected the Inside Airbnb dataset to analyze Airbnb's impact on housing markets and communities in cities globally. The project aims to provide an independent perspective, helping the public, researchers, and policymakers understand how Airbnb affects urban housing affordability and community dynamics. This data offers insights for policy discussions and social understanding of Airbnb's role in urban environments.


## 3. How was the data collected? 

1.  [\*listings.csv](http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz) : Inside Airbnb collects its data primarily by scraping information from the Airbnb website. This process involves the following steps: i.Web Scraping: Inside Airbnb employs scripts to rapidly and extensively extract Airbnb listing data, mimicking human browsing. ii.Data Extraction: Information about each listing, such as location, price, availability, number of bedrooms, reviews, and host details, is extracted and compiled. iii.Data Aggregation: Aggregated data forms a database for analyzing Airbnb trends and insights across cities and regions. iv.Regular Updates: The scraping process is repeated periodically to keep the database current, capturing new listings and updates to existing ones. v.Public Accessibility: Aggregated data is often made available to the public via the Inside Airbnb website, but the web scraping it employs may face legal and ethical challenges due to website terms of service and regional data privacy and usage laws.
2.  [\*London_Boroughs.gpkg](https://data.london.gov.uk/download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f/London_Boroughs.gpkg) : Boundary-Line for England and Wales was digitized from Ordnance Survey sheets at 1:10,000 scale, with GSS codes from ONS and GROS. GIS software manages this data, ideal for developing applications and compatible with other digital mapping systems. It's coordinated on the National Grid for easy data superimposition.

## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise? 

For the listings.csv file, its data is mainly obtained by scraping information from the Airbnb website, so it may be limited by the range and depth of information publicly available on the site. For instance, detailed information about certain listings might not be fully disclosed, or website terms might restrict access to some data. Moreover, legal and ethical considerations in web scraping, such as data privacy and usage rights, may affect the integrity and accuracy of the data. The content of the website is constantly changing dynamically, but data scraping occurs at intervals, which means the data might not be updated in real time, potentially leading to information gaps (Prentice and Pawlicz, 2023). Regarding the London_Boroughs.gpkg file, the method of data extraction relies on precise Geographic Information System (GIS) technology and detailed national geographic data. While this approach usually provides high accuracy and quality data, there may be limitations in terms of update frequency, geographic data coverage, and level of detail. Moreover, the processing and management of such data require specialized GIS technology and knowledge, which may limit the broad use and interpretation of the data.

## 5. What ethical considerations does the use of this data raise?

(1)Privacy issues: whether the dataset has the consent of the landlord owner to disclose its information, e.g., house location, name. Geocoded data is privacy-sensitive and highly likely to expose personal privacy when used to study demographic patterns and behaviours[@van_den_bemt_teaching_2018]. Therefore, obtaining the user's consent is crucial to whether the user's privacy is effectively safeguarded.

(2)Legal compliance: whether the use of the dataset complies with laws and regulations such as GDPR, DPA and EDPS.The EDPS 2015 report states that it is not enough to comply with the law in today's digital environment; we must consider the ethical dimensions of data processing.[@hasselbalch_making_2019] Therefore, legal compliance and ethical considerations should be combined in the digital age.

(3)Social responsibility: it is important to use datasets appropriately, as publishing certain data may exacerbate behaviours such as inequality and bias. The Fairness and Openness Report emphasizes how to use information responsibly and ethically, and it is quite crucial to resist the labelling of low-income communities, people of colour, etc. For example, significant differences in housing prices in different neighbourhoods may reflect economic disparities, which can affect perceptions of the socioeconomic status of those areas. Therefore, there is a need to consider how to disclose the labelled attributes of the data in question to avoid negative impacts.

(4)Data security: Some sensitive information in the dataset, such as personal information descriptions and geographic coordinates, must be stored securely to prevent access and misuse by unauthorized persons, thus avoiding security risks such as identity theft and property loss. By adjusting the norms of the use of network data, it is possible to effectively guarantee data security and increase the level of ethical behaviour of companies when processing data[@СкибунО.Ж:2022]. Thus, attention to data security can prevent the conduct of unscrupulous individuals who collect housing data for profit or surveillance purposes.

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London?

### 6.1 Why should we choose the textual information?

Many studies have analyzed various aspects of Airbnb listings, including price[@KeyFactorsAffecting], spatial distribution[@LocationofAirbnbandhotels], room type, etc. However, the "textual description", with more impressive potential than numeric fields, also plays a crucial role in shaping renters' first impressions of the listings, contributing to facilitating successful rental transactions. Additionally, hosts would also focus on the feedback of market demands to adjust their descriptions in response to policy requirements or economic trends.

Therefore, we have the reasonable motivation to scrutinize the textual features/characteristics from the data, aiming to generalize, classify and summarize some insightful conclusion. After correlating the insights with rental potential value, we hope to obtain valuable information about the short-term rental industry based on boroughs or wards as the basic geographical units.

### 6.2 What can we dig from the textual information?

Datasets consists of two textual fields: 'Description' and 'Amenities' both from the host's subjective statement as self-promotion. 'description' column is some sentences describing listing's advantages xxxxxx. 'Amenities' column is bunch of facilities and amenities inside of affiliated with the listing.

After some cleaning and preprocessing of the dataset, there are two set of questions corresponding to the two columns respectively.

#### Which topics would host like to focus on when promoting their properties?

We could use the LDA model to generalize and extract topics to get the most frequent keywords in those topics. Firstly, we need to calculate iteratively the coherence of the LDA model with the number of topics ranging from 1 to 40, in order to determine the most appropriate number of topics for summarizing the hosts' textual descriptions (@Figure1a).

```{python}

coherence_url = "https://raw.githubusercontent.com/BohaoSuCC/Groupwork_DeskB/main/Data/coherence_values.csv"

# create local path for saving
local_coherence_path = os.path.join(current_dir, "Data","coherence_values.csv")

# download and save .bib
response = requests.get(coherence_url)
with open(local_coherence_path, 'wb') as file:
    file.write(response.content)

# because it might cost several minutes to run the LDA modle
# so we just directly read the model's output remotely
# the detailed coding info could be accessed through project's github
LDAtopicwords_url = "https://raw.githubusercontent.com/BohaoSuCC/Groupwork_DeskB/main/Data/lda_topics_and_words.csv"

# create local path for saving
local_LDAtopicwords_path = os.path.join(current_dir, "Data","lda_topics_and_words.csv")

# download and save .bib
response = requests.get(LDAtopicwords_url)
with open(local_LDAtopicwords_path, 'wb') as file:
    file.write(response.content)
```

```{python}

# read coherence,csv
LDA_topic_coherence_frame = pd.read_csv(os.path.join("Data","coherence_values.csv"))

# read the LDA model output
LDA_topics_and_words_frame = pd.read_csv(os.path.join("Data","lda_topics_and_words.csv"))
```

```{python}
# create the line chart
fig, ax1 = plt.subplots(figsize=(12, 4))
ax1.plot(LDA_topic_coherence_frame['Topic_Num'], LDA_topic_coherence_frame['Coherence_Score'], marker='o')
ax1.set_title('Coherence Scores across Different Numbers of Topics')
ax1.set_xlabel('Number of Topics')
ax1.set_ylabel('Coherence Score')
ax1.grid(True)

# add the label for Y value
for x, y in zip(LDA_topic_coherence_frame['Topic_Num'], LDA_topic_coherence_frame['Coherence_Score']):
    ax1.annotate(f'{y:.3f}', (x, y), textcoords="offset points", xytext=(0, 5), ha='center')

plt.show()

```

#+ label: Figure1a
#+ fig.cap: "Figure1a: best number of topics for summarizing key words"

```{python}

fig, axes = plt.subplots(4, 4, figsize=(24, 24))  

axes = axes.flatten()  

# plot wordcloud for each topic
for i, topic in enumerate(LDA_topics_and_words_frame['Topic'].unique()):
    topic_data = LDA_topics_and_words_frame[LDA_topics_and_words_frame['Topic'] == topic]
    word_frequencies = {row['Word']: row['Weight'] for index, row in topic_data.iterrows()}

    wordcloud = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(word_frequencies)
    axes[i].imshow(wordcloud, interpolation='bilinear')
    axes[i].axis('off')
    axes[i].set_title(f'Topic {topic}')


plt.tight_layout()
plt.show()
```

#+ label: Figure1b
#+ fig.cap: "Figure1b: Topics and key words"

```{python}
"""
# 设置大图的大小
fig = plt.figure(figsize=(24, 12))

# 创建GridSpec布局，分配不同的宽度比例给折线图和词云图
gs = GridSpec(1, 2, width_ratios=[1, 3], figure=fig)  # 假设您希望词云图占更多空间

# 第一个子图：折线图
ax1 = fig.add_subplot(gs[0, 0])
ax1.plot(LDA_topic_coherence_frame['Topic_Num'], LDA_topic_coherence_frame['Coherence_Score'], marker='o')
ax1.set_title('Coherence Scores across Different Numbers of Topics')
ax1.set_xlabel('Number of Topics')
ax1.set_ylabel('Coherence Score')
ax1.grid(True)
for x, y in zip(LDA_topic_coherence_frame['Topic_Num'], LDA_topic_coherence_frame['Coherence_Score']):
    ax1.annotate(f'{y:.3f}', (x, y), textcoords="offset points", xytext=(0,5), ha='center')

# 第二个子图区域：词云图
wordcloud_gs = GridSpec(4, 4, gs[0, 1])

# 为每个主题生成词云图，并放置在wordcloud_gs网格中
for i, topic in enumerate(LDA_topics_and_words_frame['Topic'].unique()):
    topic_data = LDA_topics_and_words_frame[LDA_topics_and_words_frame['Topic'] == topic]
    word_frequencies = {row['Word']: row['Weight'] for index, row in topic_data.iterrows()}

    ax = fig.add_subplot(wordcloud_gs[i])
    wordcloud = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(word_frequencies)
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    ax.set_title(f'Topic {topic}')

# 手动调整子图间距
fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.1, hspace=0.1)

plt.show()
"""
```

Then, words clouds shows that among 16 topics, there are some topics mainly describe the location of areas, just like topic8 and topic6. Also, there are also some containing information about the facilities and some adjectives towards surrounding environments like topic13 and topic14. In short, all of those key words could illustrate the general features about what do most of Airbnb listings look like. Additionally, this key words could be of great value to the recommendation algorithms in platform's branding.

#### Do the listings in the same neighbourhood, or with the same spatial location, share the similar amenities?

Amenities are highly categorizable, meaning lots of similarities, like '500Mb WiFi' and 'highspeed Internet access'. Thus, we need to identify the similarities among various amenities and appropriately categorize them, just like group all the synonyms from a vast vocabulary.We use the Word2Vec model to classify a large number of words and phrases, each represented as a multi-dimensional vector. We then apply UMAP to reduce their dimensions to two for better visualization (Figure 2).

```{python}
# import Word2Vec Model remotely
word2vec_url = "https://github.com/BohaoSuCC/Groupwork_DeskB/raw/main/Model/word2vec-d500-w40.model"

# create local path for saving
local_word2vec_path = os.path.join(current_dir, "Model","word2vec.model")

# download and save .bib
response = requests.get(word2vec_url)
with open(local_word2vec_path, 'wb') as file:
    file.write(response.content)


word2vec_model = Word2Vec.load(os.path.join("Model","word2vec.model"))
```

```{python}
# read csv after norm and split
amenities_norm_split = pd.read_csv("https://raw.githubusercontent.com/BohaoSuCC/Groupwork_DeskB/main/Data/amenities_norm_split.csv",low_memory=False)

```

```{python}

NormListing_url = "https://github.com/BohaoSuCC/Groupwork_DeskB/raw/main/Data/Airbnb_listing_norm_min.zip"

local_NormListing_path = os.path.join(data_dir, "Airbnb_listing_norm_min.zip")

response = requests.get(NormListing_url)
with open(local_NormListing_path, 'wb') as file:
    file.write(response.content)

with zipfile.ZipFile(local_NormListing_path, 'r') as zip_ref:
    zip_ref.extractall(data_dir)

Airbnb_Listing = pd.read_csv(os.path.join("Data","Airbnb_listing_norm_min.csv"))

```

```{python}

Airbnb_Listing = pd.read_csv(os.path.join("Data","Airbnb_listing_norm_min.csv"))

texts_word2vec = Airbnb_Listing['amenities_norm']


# 将 'amenities' 列中的字符串转换为列表
# 使用 ast.literal_eval 安全地评估字符串表达的列表
amenities_ast_literal = amenities_norm_split
amenities_ast_literal.drop('Unnamed: 0',axis=1)


list_of_lists = amenities_ast_literal.apply(lambda row: [item for item in row if item is not None], axis=1).tolist()

```

```{python}

# 向量化函数
def vectorize(text, model):
    # 将文本分解为单词，并过滤掉模型词汇表中不存在的单词
    words = [word for word in text if word in model.wv.key_to_index]
    # 如果文本中没有模型已知的单词，则返回零向量
    if len(words) == 0:
        return np.zeros(model.vector_size)
    # 计算所有单词向量的平均值
    word_vectors = [model.wv[word] for word in words]
    return np.mean(word_vectors, axis=0)

Airbnb_Listing['amenities_vector'] = pd.Series(list_of_lists).apply(lambda x: vectorize(x, word2vec_model))

amenities_vector = Airbnb_Listing['amenities_vector']
```

```{python}
# UMAP降维可视化word2vec模型 
# 将pd.Series转换为np.array形式
amenities_vector_nparray = amenities_vector.to_numpy()

numpy_array = np.array([np.array(x) for x in amenities_vector_nparray])

reducer = umap.UMAP(n_components=2,n_neighbors=10,min_dist=0.9)
embedding = reducer.fit_transform(numpy_array)

```

```{python}
# 计算所有点的中心坐标
center = np.median(embedding, axis=0)

# 计算平移量
translation = -center

# 对所有点应用平移变换
translated_embedding = embedding + translation

# 验证新的中心点
new_center = translated_embedding.mean(axis=0)
print(f"New center after translation: {new_center}")
```
```{python}
mag = np.sqrt(np.power(translated_embedding[:,0],2) + np.power(translated_embedding[:,1],2)).reshape(-1,1)
angle = np.arctan2(translated_embedding[:,1], translated_embedding[:,0])

#对角度和距离进行归一化处理
angle = (angle-np.min(angle)) / (np.max(angle) - np.min(angle))
#最大最小值缩放到[0,1]
#mag = (mag-np.min(mag)) / (np.max(mag)-np.min(mag))

#标准化后缩放到[0,1]
mag = (mag-np.mean(mag)/np.std(mag))
#mag = (mag-np.min(mag)) / (np.max(mag)-np.min(mag))

#tanh缩放，正切双曲线
#mag = (np.tanh(mag)+1)/2

#sigmoid缩放
mag = 1 / (1 + np.exp(-mag))

circ_colors = mpl.colors.hsv_to_rgb(np.concatenate((angle.reshape(-1,1),  
                                                    np.ones_like(mag).reshape(-1,1), 
                                                    mag.reshape(-1,1)),
                                                    axis=1))

plt.savefig(os.path.join("Images","Word2Vec_2D_UMAP_Projection.png"), dpi=300)

color_info = np.concatenate((translated_embedding, circ_colors), axis=1)


fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(translated_embedding[:, 0], translated_embedding[:, 1], color=circ_colors, s=0.5);
ax.axis('off');
fig.savefig(os.path.join("Images","Word2Vec_2D_UMAP_Projection.png"), dpi=300)
```
#+ label: Figure2a
#+ fig.cap: "Figure2a: Features clustering after UMAP"
```{python}

# 将颜色信息保存至Airbnb_Listing

Airbnb_Listing['Word2Vec_UMAP_Xcor'] = color_info[:, 0]  # 第一列
Airbnb_Listing['Word2Vec_UMAP_Ycor'] = color_info[:, 1]  # 第二列
Airbnb_Listing['Word2Vec_UMAP_colorR'] = color_info[:, 2]  # 第三列
Airbnb_Listing['Word2Vec_UMAP_colorG'] = color_info[:, 3]  # 第四列
Airbnb_Listing['Word2Vec_UMAP_colorB'] = color_info[:, 4]  # 第五列

Airbnb_Listing.to_csv(os.path.join("Data","Airbnb_Ame2Vec_UMAP2color.csv"))

```
```{python}

Airbnb_Listing = pd.read_csv(os.path.join("Data","Airbnb_Ame2Vec_UMAP2color.csv"),low_memory=False)

```

```{python}

# Transfer pandas dataframe (Airbnb_listing.csv) to geopandas geodataframe
# By using the coordinates ()

# Converting to GeoDataframe
gdf_listing = gpd.GeoDataFrame(Airbnb_Listing, geometry=gpd.points_from_xy(Airbnb_Listing.longitude, Airbnb_Listing.latitude))

# Set the CRS
gdf_listing.set_crs("EPSG:4326", inplace=True)  # (EPSG:4326)

print("Converting successful")

# Drop NAs of columns ['amenities_norm','longitude','latitude']
gdf_listing = gdf_listing.dropna(subset=['amenities_norm','longitude','latitude'])
print(f"Now gdf has {gdf_listing.shape[0]:,} rows and {gdf_listing.shape[1]:,} columns.")

gdf_listing = gdf_listing.to_crs(epsg=3857)
London_boroughs = London_boroughs.to_crs(epsg=3857)
London_wards = London_wards.to_crs(epsg=3857)
print("gdf_listing CRS:", gdf_listing.crs)
print("London_boroughs CRS:", London_boroughs.crs)
print("London_wards CRS:", London_wards.crs)

# 绘制地图
fig, ax = plt.subplots(figsize=(16, 16))

London_boroughs.boundary.plot(ax=ax, edgecolor='black', linewidth=1, alpha=0.4)
London_wards.boundary.plot(ax=ax, edgecolor='black', linewidth=1, alpha=0.2)

# 从 gdf_listing 提取坐标点和 RGB 颜色值
x = gdf_listing.geometry.x  # 经度
y = gdf_listing.geometry.y  # 纬度
colors = gdf_listing[['Word2Vec_UMAP_colorR', 'Word2Vec_UMAP_colorG', 'Word2Vec_UMAP_colorB']].values  # RGB 颜色值

# 增加colors的亮度（通过增加 50% 的亮度）
brightness_factor = 1.5
colors_brightened = np.clip(colors * brightness_factor, 0, 1)  # 确保值在 0 到 1 之间

# 绘制散点图
ax.scatter(x, y, color=colors_brightened, s=40, alpha=0.1)  # 假设 RGB 值在 0-255 范围内

# add the label for boroughs
for idx, row in London_boroughs.iterrows():
    centroid = row.geometry.centroid
    text = ax.text(centroid.x, centroid.y, row['name'], fontsize=9, color='white',ha='center', va='center', alpha=0.7,
                   path_effects=[PathEffects.withStroke(linewidth=0.5, foreground='black')])

# add the label for wards
for idx, row in London_wards.iterrows():
    centroid = row.geometry.centroid
    text = ax.text(centroid.x, centroid.y, row['NAME'], fontsize=2, color='white',ha='center', va='center', alpha=0.7,
                   path_effects=[PathEffects.withStroke(linewidth=0.5, foreground='black')])
 
#OSM彩色底图,透明图0.7
ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, alpha = 0.7)              

# 隐藏子坐标轴的坐标轴
ax_inset.axis('off')

# 隐藏坐标轴
ax.xaxis.set_visible(False)
ax.yaxis.set_visible(False)

plt.show()

# 保存为 PNG 文件，分辨率为 300 dpi
fig.savefig(os.path.join("Images","Word2Vec_OSM_geospace.png"), dpi=300)  
```
#+ label: Figure2b
#+ fig.cap: "Figure2b: spatial distribution of Listing's similarities"

In the [@Figure2b], each point represents the amenities feature of a property, and points with similar colors indicate highly similar amenities features between properties. Afterward, we reposition these points on a map according to their actual geographical locations. This allows us to determine whether the properties in a specific area or community exhibit homogeneity (highly similar colors) or heterogeneity (more varied colors) in terms of amenities features.

### 6.3 Which indicator guide the branding?

Even though Airbnb,as a responsible company, should take community and regulation into consideration, the essence of branding and recommendation system is aiming to increase profit rate. Therefore comes the value question: what indicator could represent the potential opportunities for listing's branding or promotion?

```{python}

Airbnb_Listing_origin = pd.read_csv(os.path.join("Data","listings.csv"),low_memory=False)

# 每个listing的收入与average收入相比
if Airbnb_Listing_origin['price'].dtype == 'object':
    Airbnb_Listing_origin['price'] = Airbnb_Listing_origin['price'].str.replace('$', '').str.replace(',', '').astype(float)
Airbnb_Listing['sum_income'] = Airbnb_Listing_origin['minimum_nights']*2.7*Airbnb_Listing_origin['number_of_reviews_ltm']*Airbnb_Listing_origin['price']

average_income_forlisting = Airbnb_Listing['sum_income'].mean()
average_income_forlisting

Airbnb_Listing['price'] = Airbnb_Listing_origin['price']

print((Airbnb_Listing['price'] >= 2000).sum())
print(f"Data frame is {Airbnb_Listing.shape[0]:,} x {Airbnb_Listing.shape[1]:,}")

# 仅保留 'price' 小于 2000 的行
Airbnb_Listing = Airbnb_Listing[Airbnb_Listing['price'] < 2000]

# 检查删除后还有dataframe的shape
print(f"Data frame is {Airbnb_Listing.shape[0]:,} x {Airbnb_Listing.shape[1]:,}")

Airbnb_Listing['profitable'] = (Airbnb_Listing['sum_income'] >= average_income_forlisting).astype(int)

median_income_forlisting = Airbnb_Listing['sum_income'].median()

# Transfer pandas dataframe (Airbnb_listing.csv) to geopandas geodataframe
# By using the coordinates ()

# Converting to GeoDataframe
gdf_listing = gpd.GeoDataFrame(Airbnb_Listing, geometry=gpd.points_from_xy(Airbnb_Listing.longitude, Airbnb_Listing.latitude))

# Set the CRS
gdf_listing.set_crs("EPSG:4326", inplace=True)  # (EPSG:4326)

print("Converting successful")

# Drop NAs of columns ['description','amenities']
gdf_listing = gdf_listing.dropna(subset=['description','amenities'])
print(f"Now gdf has {gdf_listing.shape[0]:,} rows and {gdf_listing.shape[1]:,} columns.")
```

```{python}
gdf_listing = gdf_listing.to_crs(epsg=3857)
London_boroughs = London_boroughs.to_crs(epsg=3857)
London_wards = London_wards.to_crs(epsg=3857)

print("gdf_listing CRS:", gdf_listing.crs)
print("London_boroughs CRS:", London_boroughs.crs)
print("London_boroughs CRS:", London_wards.crs)

# 添加 borough 名称
gdf_listing_with_borough = gpd.sjoin(gdf_listing, London_boroughs, how='left', op='within')
gdf_listing_with_borough = gdf_listing_with_borough.rename(columns={'name': 'borough_name'})

# 添加 ward 名称
gdf_listing_with_borough_wards = gpd.sjoin(gdf_listing_with_borough, London_wards, how='left', op='within')
gdf_listing_with_borough_wards = gdf_listing_with_borough_wards.rename(columns={'name': 'ward_name'})

# 查看结果
print(gdf_listing_with_borough_wards.head())

gdf_listing['log_sum_income'] = np.log(gdf_listing['sum_income'])

gdf_listing['log_sum_income'].value_counts()
gdf_listing_dropinf = gdf_listing[gdf_listing['log_sum_income'] != -np.inf]
```

```{python}
gdf_listing_dropinf['log_sum_income'].hist(bins=150)

# 首先，将数据归一化到 0 和 1 之间
min_val = gdf_listing_dropinf['log_sum_income'].min()
max_val = gdf_listing_dropinf['log_sum_income'].max()
gdf_listing_dropinf['log_sum_income_normalized'] = (gdf_listing_dropinf['log_sum_income'] - min_val) / (max_val - min_val)


# 然后，将 0-1 范围调整到 -1 到 1
gdf_listing_dropinf['log_sum_income_normalized_scaled'] = gdf_listing_dropinf['log_sum_income_normalized'] * 2 - 1

median_num_income = np.median(gdf_listing_dropinf['log_sum_income_normalized_scaled'],axis=0)
gdf_listing_dropinf['log_sum_income_normalized_scaled'] = gdf_listing_dropinf['log_sum_income_normalized_scaled'] - median_num_income

gdf_listing_dropinf['log_sum_income_normalized_scaled'].hist(bins=150)
```

```{python}
###relating coding for PART6

# 绘制地图
fig, ax = plt.subplots(figsize=(16, 16))

# 计算 Jenks 自然断点
#breaks = jenkspy.jenks_breaks(gdf_listing_dropinf['log_sum_income_normalized_scaled'],n_classes=15)

# 手动指定断点
breaks = [-1,-0.75,-0.5,-0.4,-0.25,-0.20,-0.10,-0.05,-0.04,-0.03,-0.02,-0.01,-0.005,0,0.005,0.01,0.02,0.03,0.04,0.05,0.10,0.20,0.25,0.4,0.5,0.75,1,2]

# 添加区域名称的文字标签
for idx, row in London_boroughs.iterrows():
    centroid = row.geometry.centroid
    text = ax.text(centroid.x, centroid.y, row['name'], fontsize=7, color='black',ha='center', va='center', alpha=0.5,
                   path_effects=[PathEffects.withStroke(linewidth=0.5, foreground='white')])

# 使用断点对数据进行分类
gdf_listing_dropinf['income_category'] = np.digitize(gdf_listing_dropinf['log_sum_income_normalized_scaled'], breaks)

#绘制borough与wards的边界
London_boroughs.boundary.plot(ax=ax, edgecolor='black', linewidth=1, alpha=0.4)
London_wards.boundary.plot(ax=ax, edgecolor='black', linewidth=0.5, alpha=0.2)

# 绘制点，根据sum_income列进行上色
# 这里假设gdf具有'geometry'列，包含点的位置
scatter = ax.scatter(gdf_listing_dropinf.geometry.x, gdf_listing_dropinf.geometry.y, 
                     c=gdf_listing_dropinf['log_sum_income_normalized_scaled'], edgecolors=None, s=40, cmap='bwr_r', 
                     vmax=0.4, vmin=-0.5, alpha=0.2)


# 添加颜色条
cbar = plt.colorbar(scatter, ax=ax, label='Income', shrink=0.5, pad=0)
# 调整颜色条的大小和位置
cbar.ax.set_aspect(20)  # 这里的数字越大，颜色条越窄

# 隐藏坐标轴
ax.xaxis.set_visible(False)
ax.yaxis.set_visible(False)

ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)         #添加OSM底图

plt.show()

# 保存为 PNG 文件，分辨率为 300 dpi
fig.savefig(os.path.join("Images","Listings_profit_ratio.png"), dpi=300)  
```

We multiple the price of each listing by its total nights in the last year, total nights was calculated by minimum nights, maximum nights and number of reviews. Though, technically this is an approximate number, but it aligns with the data from the Inside Airbnb. Afterwards, we compare 'sum_income' with the average property value to get an integrated index to indicate this listing's 'cost-benefit ratio'.

### 6.4 How does the indicator correlate with textual information?

By using the SVM to train the model for better predicting the indicator X according to the textual information, we could apply this method and get an [\*trained model with accuracy more than 85%](https://github.com/BohaoSuCC/Groupwork_DeskB/blob/main/Processing_Modeling/Word2Vec_Modelling_by_SVM.ipynb), which could help the Airbnb platform or Government to assess and evaluate the listings before they are promoted and recommended to the potential renters. The SVM model is not easy to visualize, yet we have the confidence that it could offer some valuable insights both for the platform and community.

```{python}
###relating coding for PART6
```

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London?

STL: In an effort to preserve the city's current housing supply, the government legalized short-term rentals in London for a maximum of 90 days per calendar year with the introduction of the 2011 Localism Act and the 2015 Deregulation Act. Nevertheless, a number of studies point out that this deadline isn't always adhered to in reality. Most of the Airbnb listings (77%) did respect the 90-day limit, which nonetheless leaves an important proportion (23%) that did not. Of the listings surpassing the 90-day limit, the average estimated occupancy was 145 nights a year. Of these lettings, 6,140 (or 55%) were entire homes and 5,000 (or 45%) were private rooms.

The majority of London Airbnb hosts (84%) manage only one listing, while 1% (280 hosts) with over 10 properties each, accounting for 15% of all active short-term lettings, are predominantly commercial entities, contradicting government policy and legislation intentions. Some 72 of these companies were found to actively encourage hosts to exceed the 90-day limit with different platforms, a practice considered illegal.

Much of the existing research has focused on the role of Airbnb as the most prominent and prevalent online platform for short-term lets in the UK and internationally. Researchers, policymakers, and the public have voiced increasing concern about the possible negative externalities caused by the exponential rise in short-term lets this past decade. Local governments, in particular, are exploring viable ways to regulate and facilitate the practice while minimizing its potential negative effects.

Enhancing Airbnb Branding:

To enhance the Airbnb platform strategically, leveraging text features for branding and recommendation algorithms is crucial. The following strategies can be implemented:

-   Homogeneous listing selection: Utilizing text features enables the screening of homogeneous listings, reducing the addition of new listings in spatially concentrated areas. This fosters a more balanced distribution of listings to improve overall equity in housing availability, while homogeneity might contribute to housing market distortions.

-   Reinforcing recommendation algorithms in low occupancy areas: In regions with lower occupancy rates, strengthening recommendation algorithms ensures a more balanced overall occupancy rate. This proactive approach mitigates vacancy concerns, contributing to the dynamic equilibrium of London's housing market and boosting hosts' profitability.

-   Positive feedback for high rental profitability listings: For listings with high rental profitability, providing additional positive feedback serves to incentivize competitive listings. This stimulates a positive feedback loop, promoting sound business operations beneficial for both Airbnb and landlords.

Government Regulatory Options:

Furukawa & Onuki's tri-categorical definition indicates that effective policies should be less restrictive for Primary Hosted & Unhosted Short-term lets within appropriate timeframes, while regulating Nonprimary short-term lets more firmly to provide the right incentives to landlords to rent long-term.

Tailored Policies Based on Spatial Distribution Features: Tailoring policies for diverse community types is essential. In high-density areas, consider limiting the addition of new listings to prevent overcrowding. In contrast, for areas with lower occupancy rates, policies can encourage landlords to adopt more proactive occupancy promotion strategies.

Dynamic Policy Adjustments for Supply-Demand Balance: Utilize spatial distribution features to monitor market dynamics and make dynamic policy adjustments based on actual demand. In high-demand areas, policies can be more flexible, encouraging short-term rentals, while in oversupplied regions, stricter policies can reduce vacancy rates. Connect the identified branding opportunities with STL regulations to strike a balance between encouraging tourism and preventing negative impacts on housing markets. For areas with distinctive amenities or features, consider regulations that preserve the uniqueness without contributing to housing shortages.

Encouraging Landlord Engagement in Community Development: Airbnb transforms residential communities into tourist spaces and changes the socio-cultural landscape of urban neighborhoods. It specifically propagates the experience of 'living like a local, but this consumption of everyday local residential life has implications for the well-being of long-term tenants, including the disruption and erasure of long-term communities and housing insecurity. Critical urbanists have accordingly linked Airbnb to touristification/gentrification - what Peters has called 'Airbnbification'. Governments can consider incentivizing landlords to participate in community development, aiming to increase the 90-day occupancy rate. This not only reduces long-term property vacancies but also fosters community vitality and helps maintain supply-demand equilibrium.

Create a Registration Service to Bridge Gaps in Data: In a context where the lack of data is cited as a major limitation in research and decision-making outcomes, a registration service could provide some of the information necessary to bridge this gap. Use statistical analysis and models to support regulatory decisions, ensuring they are evidence-based and grounded in the specific characteristics of each area. Incorporate figures, maps, and statistical insights to demonstrate the impact of STL on housing prices, availability, and community well-being.

## References
