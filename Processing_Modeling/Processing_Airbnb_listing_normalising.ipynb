{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the libray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import unicodedata\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "import contextily as ctx\n",
    "import urllib.request\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder  # We don't use this but I point out where you *could*\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.models import tfidfmodel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Import everthing from textual/__init__.py\n",
    "# Including bunch of tools and functions we could use for NLP \n",
    "from textual import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and read the csv file remotely from url\n",
    "host = 'http://data.insideairbnb.com'\n",
    "path = 'united-kingdom/england/london/2023-09-06/data'\n",
    "file = 'listings.csv.gz'\n",
    "url  = f'{host}/{path}/{file}'\n",
    "\n",
    "# Save csv file\n",
    "if os.path.exists(file):\n",
    "  Airbnb_Listing = pd.read_csv(file, compression='gzip', low_memory=False)\n",
    "else: \n",
    "  Airbnb_Listing = pd.read_csv(url, compression='gzip', low_memory=False)\n",
    "  Airbnb_Listing.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# Prepare for necessary nltk packages\n",
    "nltk.download('wordnet') # <-- These are done in a supporting tool, but in your own\n",
    "nltk.download('averaged_perceptron_tagger') # application you'd need to import them\n",
    "nltk.download('stopwords')\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "print(\"nltk.download successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select useful columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airbnb_Listing.columns\n",
    "remained_columns = ['id','description','neighbourhood_cleansed','latitude','longitude',\n",
    "                    'room_type','amenities','price','number_of_reviews',\n",
    "                    'review_scores_rating', 'review_scores_accuracy','review_scores_cleanliness', \n",
    "                    'review_scores_checkin','review_scores_communication', 'review_scores_location','review_scores_value',\n",
    "                    'reviews_per_month']\n",
    "Airbnb_Listing = Airbnb_Listing[remained_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing data for normalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relating operating tools to normalise 'description' and 'amenities' from Jon Reades codes\n",
    "\n",
    "\n",
    "host  = 'https://orca.casa.ucl.ac.uk'\n",
    "turl  = f'{host}/~jreades/__textual__.py'\n",
    "tdirs = os.path.join('textual')\n",
    "tpath = os.path.join(tdirs,'__init__.py')\n",
    "\n",
    "if not os.path.exists(tpath):\n",
    "    os.makedirs(tdirs, exist_ok=True)\n",
    "    urllib.request.urlretrieve(turl, tpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Drop NAs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs of columns ['description','amenities']\n",
    "Airbnb_Listing = Airbnb_Listing.dropna(subset=['description','amenities'])\n",
    "print(f\"Now gdf has {Airbnb_Listing.shape[0]:,} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codes below should be operated only once, run it again only if updating needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time \n",
    "# I get about 21 minutes \n",
    "Airbnb_Listing['description_norm'] = Airbnb_Listing['description'].apply(normalise_document, remove_digits=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time \n",
    "# Codes below should be operated only once, run it again only if undating needed\n",
    "\n",
    "# I get about 21 minutes \n",
    "Airbnb_Listing['amenities_norm'] = Airbnb_Listing['amenities'].apply(normalise_document, remove_digits=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing finished and Saving csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes below should be operated only once, run it again only if undating needed\n",
    "# Airbnb_Listing.to_csv('./Data/Airbnb_Listing_norm.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
