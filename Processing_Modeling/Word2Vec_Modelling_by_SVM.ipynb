{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SBH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import unicodedata\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "import contextily as ctx\n",
    "import urllib.request\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder  # We don't use this but I point out where you *could*\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.models import tfidfmodel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopword_list = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从norm后的数据读取csv\n",
    "Airbnb_Listing = pd.read_csv(os.path.join(\"..\",\"Data\",'Airbnb_Listing_norm.csv'))\n",
    "\n",
    "# 假设 `texts` 是你的分词后的文本数据（每个文本为词的列表）\n",
    "texts_word2vec = Airbnb_Listing['amenities']  # 你的分词数据\n",
    "\n",
    "# 指定训练参数\n",
    "dims = 400\n",
    "window = 10\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "model = Word2Vec(sentences=texts_word2vec, vector_size=dims, window=window, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Model\\\\word2vec-d400-w10.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:764\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     _pickle\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, fname_or_handle, protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n\u001b[0;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SBH\\OneDrive - University College London\\#CASA0013__FoundationsOfSpatialDataScience\\Groupwork_DeskB\\Processing_Modeling\\Word2Vec_Modelling_by_SVM.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0013__FoundationsOfSpatialDataScience/Groupwork_DeskB/Processing_Modeling/Word2Vec_Modelling_by_SVM.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 保存word2vec模型\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0013__FoundationsOfSpatialDataScience/Groupwork_DeskB/Processing_Modeling/Word2Vec_Modelling_by_SVM.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m\"\u001b[39;49m\u001b[39mModel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mword2vec-d\u001b[39;49m\u001b[39m{\u001b[39;49;00mdims\u001b[39m}\u001b[39;49;00m\u001b[39m-w\u001b[39;49m\u001b[39m{\u001b[39;49;00mwindow\u001b[39m}\u001b[39;49;00m\u001b[39m.model\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:1912\u001b[0m, in \u001b[0;36mWord2Vec.save\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1902\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save the model.\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[39m    This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\u001b[39;00m\n\u001b[0;32m   1904\u001b[0m \u001b[39m    online training and getting vectors for vocabulary words.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1910\u001b[0m \n\u001b[0;32m   1911\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1912\u001b[0m     \u001b[39msuper\u001b[39;49m(Word2Vec, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:767\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    766\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:611\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    607\u001b[0m restores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_specials(\n\u001b[0;32m    608\u001b[0m     fname, separately, sep_limit, ignore, pickle_protocol, compress, subname,\n\u001b[0;32m    609\u001b[0m )\n\u001b[0;32m    610\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     pickle(\u001b[39mself\u001b[39;49m, fname, protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n\u001b[0;32m    612\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    613\u001b[0m     \u001b[39m# restore attribs handled specially\u001b[39;00m\n\u001b[0;32m    614\u001b[0m     \u001b[39mfor\u001b[39;00m obj, asides \u001b[39min\u001b[39;00m restores:\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:1442\u001b[0m, in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpickle\u001b[39m(obj, fname, protocol\u001b[39m=\u001b[39mPICKLE_PROTOCOL):\n\u001b[0;32m   1430\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \n\u001b[0;32m   1432\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \n\u001b[0;32m   1441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(fname, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fout:  \u001b[39m# 'b' for binary, needed on Windows\u001b[39;00m\n\u001b[0;32m   1443\u001b[0m         _pickle\u001b[39m.\u001b[39mdump(obj, fout, protocol\u001b[39m=\u001b[39mprotocol)\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[0;32m    189\u001b[0m     uri,\n\u001b[0;32m    190\u001b[0m     mode,\n\u001b[0;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[0;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Model\\\\word2vec-d400-w10.model'"
     ]
    }
   ],
   "source": [
    "# 保存word2vec模型\n",
    "model.save(os.path.join(\"Model\",f\"word2vec-d{dims}-w{window}.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airbnb_Listing_origin = pd.read_csv(\"./Data/Data_InsideAirbnb/listings.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算所有listing的average incom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个listing的收入与average收入相比\n",
    "if Airbnb_Listing_origin['price'].dtype == 'object':\n",
    "    Airbnb_Listing_origin['price'] = Airbnb_Listing_origin['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "Airbnb_Listing['sum_income'] = Airbnb_Listing_origin['minimum_nights']*2.7*Airbnb_Listing_origin['number_of_reviews_ltm']*Airbnb_Listing_origin['price']\n",
    "\n",
    "average_income_forlisting = Airbnb_Listing['sum_income'].mean()\n",
    "average_income_forlisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airbnb_Listing['profitable'] = (Airbnb_Listing['sum_income'] >= average_income_forlisting).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换文本向量\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # 移除不在词汇表中的词\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    # 处理空文档的情况\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    # 计算均值向量\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# 为每个文档计算向量\n",
    "doc_vectors = np.array([document_vector(model, doc) for doc in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词处理：您使用的是 text.split(\" \") 来分词。这意味着您假设文本中的每个单词之间由两个空格分隔。请确保这与您的数据格式一致。如果是普通英文文本，通常单词之间只有一个空格，那么应该使用 text.split()。\n",
    "\n",
    "空文档处理：在 document_vector 函数中，如果文档中所有的词都不在模型的词汇表中，那么 word2vec_model.wv[doc] 将是一个空列表，这会导致 np.mean 报错。您需要处理这种情况。\n",
    "\n",
    "文档向量计算：当您计算文档向量时，您使用的是 np.array([document_vector(model, doc) for doc in texts])。这里 texts 应该是分词后的文本数据。请确保 texts 和 texts_word2vec 是一致的，即 texts 应该是用于训练 Word2Vec 模型的相同数据。\n",
    "\n",
    "标签和特征数据：确保 labels 是与 doc_vectors 对应的目标变量数组。labels 应该有与 doc_vectors 相同数量的元素。\n",
    "\n",
    "模型性能评估：在最后，您计算了准确率，这是评估分类模型性能的一个常用指标。根据您的应用情况，可能还需要考虑其他指标，如精确率、召回率和F1分数。\n",
    "\n",
    "异常和错误处理：在实际应用中，建议添加异常处理和错误检查，确保代码的健壮性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林方法\n",
    "\n",
    "#使用任何类型的分类器来预测是否income超过平均值\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, Airbnb_Listing['profitable'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练分类器\n",
    "classifier = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM方法\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, Airbnb_Listing['profitable'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建 SVM 分类器实例\n",
    "svm_classifier = SVC(random_state=42)\n",
    "\n",
    "# 训练分类器\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存SVM模型与保存后的Airbnb_Listing_norm_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(svm_classifier, 'svm_classfier_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
