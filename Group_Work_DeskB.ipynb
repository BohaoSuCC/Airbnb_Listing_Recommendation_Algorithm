{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7601c72b",
   "metadata": {},
   "source": [
    "---\n",
    "bibliography: bio.bib\n",
    "csl: harvard-cite-them-right.csl\n",
    "title: DeskB's Group Project\n",
    "execute:\n",
    "  echo: false\n",
    "format:\n",
    "  html:\n",
    "    theme:\n",
    "      - minty\n",
    "      - css/web.scss\n",
    "    code-copy: true\n",
    "    code-link: true\n",
    "    toc: true\n",
    "    toc-title: On this page\n",
    "    toc-depth: 2\n",
    "    toc_float:\n",
    "      collapsed: false\n",
    "      smooth_scroll: true\n",
    "  pdf:\n",
    "    include-in-header:\n",
    "      text: |\n",
    "        \\addtokomafont{disposition}{\\rmfamily}\n",
    "    mainfont: Spectral\n",
    "    sansfont: Roboto\n",
    "    monofont: JetBrainsMono-Regular\n",
    "    papersize: a4\n",
    "    geometry:\n",
    "      - top=25mm\n",
    "      - left=40mm\n",
    "      - right=30mm\n",
    "      - bottom=25mm\n",
    "      - heightrounded\n",
    "    toc: false\n",
    "    number-sections: false\n",
    "    colorlinks: true\n",
    "    highlight-style: github\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336c31",
   "metadata": {},
   "source": [
    "## Declaration of Authorship {.unnumbered .unlisted}\n",
    "\n",
    "We, \\[DeskB\\], confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\n",
    "\n",
    "Date: 11th December 2023\n",
    "\n",
    "Student Numbers: 20017359 23032922 23081403 23103585 23130397\n",
    "\n",
    "## Brief Group Reflection\n",
    "\n",
    "| What Went Well | What Was Challenging |\n",
    "|----------------|----------------------|\n",
    "| A              | B                    |\n",
    "| C              | D                    |\n",
    "\n",
    "## Priorities for Feedback\n",
    "\n",
    "Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?\n",
    "\n",
    "\n",
    "\n",
    "```{=html}\n",
    "<style type=\"text/css\">\n",
    ".duedate {\n",
    "  border: dotted 2px red; \n",
    "  background-color: rgb(255, 235, 235);\n",
    "  height: 50px;\n",
    "  line-height: 50px;\n",
    "  margin-left: 40px;\n",
    "  margin-right: 40px\n",
    "  margin-top: 10px;\n",
    "  margin-bottom: 10px;\n",
    "  color: rgb(150,100,100);\n",
    "  text-align: center;\n",
    "}\n",
    "</style>\n",
    "```\n",
    "\n",
    "{{< pagebreak >}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Response to Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9395ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # We don't use this but I point out where you *could*\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Result!\n",
       "\n",
       ">Here's my output...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some Configuration allowing us to output Markdown (like this cell) instead of plain text\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def as_markdown(head='', body='Some body text'):\n",
    "    if head != '':\n",
    "        display_markdown(f\"##### {head}\\n\\n>{body}\\n\", raw=True)\n",
    "    else:\n",
    "        display_markdown(f\">{body}\\n\", raw=True)\n",
    "\n",
    "as_markdown('Result!', \"Here's my output...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acc8fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and read the csv file remotely from url\n",
    "host = 'http://data.insideairbnb.com'\n",
    "path = 'united-kingdom/england/london/2023-09-06/data'\n",
    "file = 'listings.csv.gz'\n",
    "url  = f'{host}/{path}/{file}'\n",
    "\n",
    "# Save csv file\n",
    "if os.path.exists(file):\n",
    "  Airbnb_Listing = pd.read_csv(file, compression='gzip', low_memory=False)\n",
    "else: \n",
    "  Airbnb_Listing = pd.read_csv(url, compression='gzip', low_memory=False)\n",
    "  Airbnb_Listing.to_csv(file)\n",
    "\n",
    "# Download and read the gpkg file remotel from url\n",
    "host = 'https://data.london.gov.uk'\n",
    "path = 'download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f'\n",
    "file = 'London_Boroughs.gpkg'\n",
    "url  = f'{host}/{path}/{file}'\n",
    "\n",
    "# Save gkpg file\n",
    "if os.path.exists(file):\n",
    "  London_boroughs = gpd.read_file(file, compression='gzip', low_memory=False)\n",
    "else: \n",
    "  London_boroughs = gpd.read_file(url, compression='gzip', low_memory=False)\n",
    "  London_boroughs.to_file(file, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05b736",
   "metadata": {},
   "source": [
    "## 1. Who collected the data? ( 2 points; Answer due Week 7 )\n",
    "\n",
    "::: 1.[\\*listings.csv](http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz) : This dataset was created by automatically scraping public information from Airbnb's Website. Murray Cox was one of the main founder and technicians of this mission driven project that aims to provide data and advocacy about Airbnb's impact on residential communities. [\\[1\\]]((http://insideairbnb.com/about))\n",
    "\n",
    "2.[\\*London_Boroughs.gpkg](https://data.london.gov.uk/download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f/London_Boroughs.gpkg) and [London-wards-2018](https://data.london.gov.uk/download/statistical-gis-boundary-files-london/08d31995-dd27-423c-a987-57fe8e952990/London-wards-2018.zip) : This dataset is an extract from [Ordnance Survey](https://www.ordnancesurvey.co.uk/) Boundary-Line product which is a specialist 1:10 000 scale boundaries dataset.\n",
    "\n",
    ":::\n",
    "\n",
    "An inline citation: As discussed on @insideairbnb, there are many...\n",
    "\n",
    "A parenthetical citation: There are many ways to research Airbnb [see, for example, @insideairbnb]...\n",
    "\n",
    "## 2. Why did they collect it? ( 4 points; Answer due Week 7 )\n",
    "\n",
    ":::\n",
    "\n",
    "1.[\\*listings.csv](http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz) : Inside Airbnb is a mission driven project that provides data and advocacy about Airbnb's impact on residential communities. We work towards a vision where communities are empowered with data and information to understand, decide and control the role of renting residential homes to tourists.\n",
    "\n",
    "2.[\\*London_Boroughs.gpkg](https://data.london.gov.uk/download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f/London_Boroughs.gpkg) : With a long history and evolving from . The Ordnance Survey aims to help governments make smarter decisions that ensure our safety and security, they also show businesses how to gain a location data edge and we help everyone experience the benefits of the world outside. Under the [Public Sector Geospatial Agreement](https://www.ordnancesurvey.co.uk/customers/public-sector/public-sector-geospatial-agreement) (PSGA), Ordnance Survey (OS) provides Great Britain' national mapping services. OS creates, maintains and provides access to consistent, definitive and authoritative location data of Great Britain, aiming to help organisations to maximise the use, value and benefit of the data for the national interest and the public good. :::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c9b5919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame is 87,946 x 75\n",
      "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
      "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
      "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
      "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
      "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
      "       'host_neighbourhood', 'host_listings_count',\n",
      "       'host_total_listings_count', 'host_verifications',\n",
      "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
      "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
      "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
      "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
      "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
      "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
      "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
      "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
      "       'availability_30', 'availability_60', 'availability_90',\n",
      "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
      "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
      "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
      "       'review_scores_cleanliness', 'review_scores_checkin',\n",
      "       'review_scores_communication', 'review_scores_location',\n",
      "       'review_scores_value', 'license', 'instant_bookable',\n",
      "       'calculated_host_listings_count',\n",
      "       'calculated_host_listings_count_entire_homes',\n",
      "       'calculated_host_listings_count_private_rooms',\n",
      "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data frame is {Airbnb_Listing.shape[0]:,} x {Airbnb_Listing.shape[1]:,}\")\n",
    "print(Airbnb_Listing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da7fa50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist_Listing = Airbnb_Listing.review_scores_rating.plot.hist(bins=10)\n",
    "plot_hist_Listing.set_xlim([0, 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695144d0",
   "metadata": {},
   "source": [
    "## 3. How was the data collected? ( 5 points; Answer due Week 8 )\n",
    "\n",
    "::: 1.[\\*listings.csv](http://data.insideairbnb.com/united-kingdom/england/london/2023-09-06/data/listings.csv.gz) : Inside Airbnb collects its data primarily by scraping information from the Airbnb website. This process involves the following steps:\n",
    "\n",
    "**i.Web Scraping**: Inside Airbnb uses automated scripts to systematically browse and extract data from Airbnb's listings. These scripts navigate the website just like a human user would, but they do it much faster and on a larger scale.\n",
    "\n",
    "**ii.Data Extraction**: Information about each listing, such as location, price, availability, number of bedrooms, reviews, and host details, is extracted and compiled.\n",
    "\n",
    "**iii.Data Aggregation**: The collected data is then aggregated into a database. This database is organized to make it easier to analyze trends, patterns, and insights related to Airbnb's offerings in various cities and regions.\n",
    "\n",
    "**iv.Regular Updates**: The scraping process is repeated periodically to keep the database current, capturing new listings and updates to existing ones.\n",
    "\n",
    "**v.Public Accessibility**: The aggregated data is often made available to the public through the Inside Airbnb website, enabling researchers, policymakers, and the general public to analyze Airbnb's impact on housing markets and communities. It's important to note that web scraping practices, like those used by Inside Airbnb, may face legal and ethical considerations depending on the website's terms of service and regional laws regarding data privacy and usage.\n",
    "\n",
    "2.[\\*London_Boroughs.gpkg](https://data.london.gov.uk/download/london_boroughs/9502cdec-5df0-46e3-8aa1-2b5c5233a31f/London_Boroughs.gpkg) : \"Boundary-Line for England and Wales was initially digitised from Ordnance Survey's boundary record sheets at 1:10 000 scale (or, in some cases, at larger scales). The Government Statistical Service (GSS) codes are supplied by the Office for National Statistics and General Register Office for Scotland(GROS). GIS software provides the functionality to store, manage and manipulate this digital map data. The properties of the data make it suitable as a key base for users wishing to develop applications. BoundaryLine is also suitable for use within other digital mapping systems. It's coordinated on the National Grid which allows for the easy superimposition of other data. :::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c406d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
      "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
      "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
      "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
      "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
      "       'host_neighbourhood', 'host_listings_count',\n",
      "       'host_total_listings_count', 'host_verifications',\n",
      "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
      "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
      "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
      "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
      "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
      "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
      "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
      "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
      "       'availability_30', 'availability_60', 'availability_90',\n",
      "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
      "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
      "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
      "       'review_scores_cleanliness', 'review_scores_checkin',\n",
      "       'review_scores_communication', 'review_scores_location',\n",
      "       'review_scores_value', 'license', 'instant_bookable',\n",
      "       'calculated_host_listings_count',\n",
      "       'calculated_host_listings_count_entire_homes',\n",
      "       'calculated_host_listings_count_private_rooms',\n",
      "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#print(Airbnb_Listing.info())\n",
    "print(Airbnb_Listing.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?\n",
    "\n",
    "::: duedate\n",
    "( 11 points; Answer due Week 9 )\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87946\n"
     ]
    }
   ],
   "source": [
    "# Check NAs 检查缺失值（缺失值会如何影响\n",
    "print(Airbnb_Listing['bathrooms'].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What ethical considerations does the use of this data raise?\n",
    "\n",
    "::: duedate\n",
    "( 18 points; Answer due {{< var assess.group-date >}} )\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87946\n"
     ]
    }
   ],
   "source": [
    "# Check NAs 检查缺失值（缺失值会如何影响\n",
    "print(Airbnb_Listing['bathrooms'].isna().sum())\n",
    "\n",
    "# Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f7f16",
   "metadata": {},
   "source": [
    "## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London?\n",
    "\n",
    "::: duedate\n",
    "``` python\n",
    "```\n",
    "\n",
    "( 15 points; Answer due {{< var assess.group-date >}} )\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义url，将名称作为key，将url链接作为value。\n",
    "#urlmap = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据基础处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk.download successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SBH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SBH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SBH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Prepare for necessary nltk packages\n",
    "nltk.download('wordnet') # <-- These are done in a supporting tool, but in your own\n",
    "nltk.download('averaged_perceptron_tagger') # application you'd need to import them\n",
    "nltk.download('stopwords')\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "print(\"nltk.download successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting successful\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# Transfer pandas dataframe (Airbnb_listing.csv) to geopandas geodataframe\n",
    "# By using the coordinates ()\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create geometry column\n",
    "geometry = [Point(xy) for xy in zip(Airbnb_Listing['longitude'], Airbnb_Listing['latitude'])]\n",
    "\n",
    "# Converting to GeoDataframe\n",
    "gdf_listing = gpd.GeoDataFrame(Airbnb_Listing, geometry=geometry)\n",
    "\n",
    "# Set the CRS\n",
    "gdf_listing.set_crs(\"EPSG:4326\", inplace=True)  # 例如，使用 WGS84 (EPSG:4326)\n",
    "\n",
    "print(\"Converting successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now gdf has 86,679 rows.\n"
     ]
    }
   ],
   "source": [
    "# Drop NAs of columns ['description','amenities']\n",
    "gdf_listing = gdf_listing.dropna(subset=['description','amenities'])\n",
    "print(f\"Now gdf has {gdf_listing.shape[0]:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relating operating tools to normalise 'description' and 'amenities' from Jon Reades codes\n",
    "import urllib.request\n",
    "host  = 'https://orca.casa.ucl.ac.uk'\n",
    "turl  = f'{host}/~jreades/__textual__.py'\n",
    "tdirs = os.path.join('textual')\n",
    "tpath = os.path.join(tdirs,'__init__.py')\n",
    "\n",
    "if not os.path.exists(tpath):\n",
    "    os.makedirs(tdirs, exist_ok=True)\n",
    "    urllib.request.urlretrieve(turl, tpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# This allows us to edit and reload the library\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everthing from textual/__init__.py\n",
    "# Including bunch of tools and functions we could use for NLP \n",
    "from textual import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying to normalise ['description','amenities']**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBH\\OneDrive - University College London\\#CASA0013__FoundationsOfSpatialDataScience\\Groupwork_DeskB\\textual\\__init__.py:606: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(doc, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 22s\n",
      "Wall time: 9min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# I get about 1 minute on a M2 Mac\n",
    "Airbnb_Listing['description_norm'] = Airbnb_Listing['description'].apply(normalise_document, remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move that column to geodataframe\n",
    "gdf_listing['description_norm'] = Airbnb_Listing['description_norm'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 0: ['space everyone  room twin people wish travel london england home rent.i live quiet leafy suburb southwest london minute waterloo station train  also several great connection  victoria  waterloo chelsea .', 'across road house also public tennis court everyone free several playing field  type sport play .', 'several restaurant nearby serve variety food  everything could wish  plus several never stick .', 'experience host really enjoy meet people .', 'irish live london year interested drop line .']\n",
      "\n",
      "Words 0: [['space', 'everyone', 'room', 'twin', 'people', 'wish', 'travel', 'london', 'england', 'home', 'rent.i', 'live', 'quiet', 'leafy', 'suburb', 'southwest', 'london', 'minute', 'waterloo', 'station', 'train', 'also', 'several', 'great', 'connection', 'victoria', 'waterloo', 'chelsea', '.'], ['across', 'road', 'house', 'also', 'public', 'tennis', 'court', 'everyone', 'free', 'several', 'playing', 'field', 'type', 'sport', 'play', '.'], ['several', 'restaurant', 'nearby', 'serve', 'variety', 'food', 'everything', 'could', 'wish', 'plus', 'several', 'never', 'stick', '.'], ['experience', 'host', 'really', 'enjoy', 'meet', 'people', '.'], ['irish', 'live', 'london', 'year', 'interested', 'drop', 'line', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Select the Corpus\n",
    "corpus = gdf_listing['description_norm'].fillna(' ').values\n",
    "\n",
    "sentences = [nltk.sent_tokenize(text) for text in corpus]\n",
    "words     = [[nltk.tokenize.word_tokenize(sentence) \n",
    "                  for sentence in nltk.sent_tokenize(text)] \n",
    "                  for text in corpus]\n",
    "\n",
    "\n",
    "print(f\"Sentences 0: {sentences[0]}\")\n",
    "print()\n",
    "print(f\"Words 0: {words[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequency and N-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 47377 samples and 5104644 outcomes>\n",
      "<FreqDist with 771940 samples and 5104643 outcomes>\n",
      "<FreqDist with 2316472 samples and 5104642 outcomes>\n",
      "           Ngram Size 1\n",
      "room              90112\n",
      "space             87019\n",
      "london            82386\n",
      "bedroom           78961\n",
      "walk              63278\n",
      "kitchen           62184\n",
      "flat              60019\n",
      "apartment         56230\n",
      "access            55755\n",
      "minute            51037\n",
      "\n",
      "                  Ngram Size 2\n",
      "minute  walk             26513\n",
      "guest   access           24663\n",
      "living  room             21831\n",
      "central london           21100\n",
      "double  bedroom          13725\n",
      "fully   equip            12454\n",
      "thing   note             11920\n",
      "tube    station           8888\n",
      "open    plan              8540\n",
      "walk    distance          8444\n",
      "\n",
      "                          Ngram Size 3\n",
      "fully   equip   kitchen           6781\n",
      "within  walk    distance          3252\n",
      "guest   access  guests            3120\n",
      "minute  walk    away              3047\n",
      "open    plan    kitchen           2959\n",
      "kitchen fully   equip             2611\n",
      "station minute  walk              2578\n",
      "guest   access  access            2386\n",
      "kitchen living  room              2247\n",
      "access  central london            2117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build counts for ngram range[1,2,3,4]\n",
    "fcounts = dict()\n",
    "\n",
    "# Here we replace all full-stops... can you think why we might do this?\n",
    "data = nltk.tokenize.word_tokenize(' '.join([text.replace('.','') for text in corpus]))\n",
    "\n",
    "for size in 1, 2, 3:\n",
    "    fdist = FreqDist(ngrams(data, size))\n",
    "    print(fdist)\n",
    "    # If you only need one note this: https://stackoverflow.com/a/52193485/4041902\n",
    "    fcounts[size] = pd.DataFrame.from_dict({f'Ngram Size {size}': fdist})\n",
    "\n",
    "# Print the most common ones for each range\n",
    "for dfs in fcounts.values():\n",
    "    print(dfs.sort_values(by=dfs.columns.values[0], ascending=False).head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary mapping for stratford is 844\n",
      "Found 1,767 rows containing stratford\n",
      "Raw count vectorised data frame has 86,679 rows and 1,000 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>access</th>\n",
       "      <th>access access</th>\n",
       "      <th>access central</th>\n",
       "      <th>access central london</th>\n",
       "      <th>access entire</th>\n",
       "      <th>access everything</th>\n",
       "      <th>access guest</th>\n",
       "      <th>access guests</th>\n",
       "      <th>access guests access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    able  access  access access  access central  access central london  \\\n",
       "0      0       0              0               0                      0   \n",
       "1      0       0              0               0                      0   \n",
       "2      0       1              0               0                      0   \n",
       "3      0       0              0               0                      0   \n",
       "4      0       0              0               0                      0   \n",
       "5      0       1              0               0                      0   \n",
       "6      0       0              0               0                      0   \n",
       "7      0       0              0               0                      0   \n",
       "8      0       2              0               0                      0   \n",
       "9      0       0              0               0                      0   \n",
       "10     0       0              0               0                      0   \n",
       "11     0       0              0               0                      0   \n",
       "12     0       1              0               0                      0   \n",
       "13     0       0              0               0                      0   \n",
       "14     0       0              0               0                      0   \n",
       "15     0       2              0               0                      0   \n",
       "16     0       0              0               0                      0   \n",
       "17     1       0              0               0                      0   \n",
       "18     0       0              0               0                      0   \n",
       "19     0       1              0               1                      1   \n",
       "\n",
       "    access entire  access everything  access guest  access guests  \\\n",
       "0               0                  0             0              0   \n",
       "1               0                  0             0              0   \n",
       "2               0                  0             0              0   \n",
       "3               0                  0             0              0   \n",
       "4               0                  0             0              0   \n",
       "5               0                  0             0              0   \n",
       "6               0                  0             0              0   \n",
       "7               0                  0             0              0   \n",
       "8               0                  0             0              0   \n",
       "9               0                  0             0              0   \n",
       "10              0                  0             0              0   \n",
       "11              0                  0             0              0   \n",
       "12              0                  0             0              0   \n",
       "13              0                  0             0              0   \n",
       "14              0                  0             0              0   \n",
       "15              0                  0             0              0   \n",
       "16              0                  0             0              0   \n",
       "17              0                  0             0              0   \n",
       "18              0                  0             0              0   \n",
       "19              0                  0             0              0   \n",
       "\n",
       "    access guests access  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "5                      0  \n",
       "6                      0  \n",
       "7                      0  \n",
       "8                      0  \n",
       "9                      0  \n",
       "10                     0  \n",
       "11                     0  \n",
       "12                     0  \n",
       "13                     0  \n",
       "14                     0  \n",
       "15                     0  \n",
       "16                     0  \n",
       "17                     0  \n",
       "18                     0  \n",
       "19                     0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set arguments for CountVectorizer\n",
    "cvectorizer = CountVectorizer(ngram_range=(1,3),max_features=1000, min_df=5, max_df=0.7)\n",
    "cvectorizer.fit(corpus)\n",
    "\n",
    "\n",
    "# Count specific term shows time in the corpus\n",
    "term = 'stratford'\n",
    "pd.options.display.max_colwidth=750\n",
    "\n",
    "# Find the vocabulary mapping for the term\n",
    "print(f\"Vocabulary mapping for {term} is {cvectorizer.vocabulary_[term]}\")\n",
    "\n",
    "# How many times is it in the data\n",
    "print(f\"Found {gdf_listing.description_norm.str.contains(term).sum():,} rows containing {term}\")\n",
    "\n",
    "# Print the descriptions containing the term\n",
    "#for x in srcdf[srcdf.description_norm.str.contains(term)].description_norm:\n",
    "    #as_markdown('Stratford',x)\n",
    "\n",
    "\n",
    "# Transform the corpus\n",
    "cvtcorpus = cvectorizer.transform(corpus)\n",
    "cvtcorpus # cvtcorpus for count-vectorised transformed corpus\n",
    "\n",
    "# The first document of the corpus\n",
    "doc_df = pd.DataFrame(cvtcorpus[0].T.todense(), \n",
    "                      index=cvectorizer.get_feature_names_out(), columns=[\"Counts\"]\n",
    "                     ).sort_values('Counts', ascending=False)\n",
    "doc_df.head(10)\n",
    "\n",
    "# Transformed the corpus\n",
    "cvdf = pd.DataFrame(data=cvtcorpus.toarray(),\n",
    "                        columns=cvectorizer.get_feature_names_out())\n",
    "print(f\"Raw count vectorised data frame has {cvdf.shape[0]:,} rows and {cvdf.shape[1]:,} columns.\")\n",
    "cvdf.iloc[0:20,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter low frequency words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the total amount of whole things in \n",
    "sums = cvdf.sum(axis=0)\n",
    "print(f\"There are {len(sums):,} terms in the data set.\")\n",
    "sums.head()\n",
    "\n",
    "filter_terms = sums >= cvdf.shape[0] * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF/IDF Vectoriser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Transform\n",
    "tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), \n",
    "                               max_df=0.75, min_df=0.01) # <-- these matter!\n",
    "tftcorpus    = tfvectorizer.fit_transform(corpus) # TF-transformed corpus\n",
    "\n",
    "# Single Document\n",
    "doc_df = pd.DataFrame(tftcorpus[0].T.todense(), index=tfvectorizer.get_feature_names_out(), columns=[\"Weights\"])\n",
    "doc_df.sort_values('Weights', ascending=False).head(10)\n",
    "\n",
    "# Transformed corpus\n",
    "tfidf = pd.DataFrame(data=tftcorpus.toarray(),\n",
    "                        columns=tfvectorizer.get_feature_names_out())\n",
    "print(f\"TF/IDF data frame has {tfidf.shape[0]:,} rows and {tfidf.shape[1]:,} columns.\")\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD COULDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcvdf.sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "#   Plotting part\n",
    "plt.figure(figsize=(12, 12))\n",
    "Cloud = WordCloud(\n",
    "    background_color=\"white\", \n",
    "    max_words=75,\n",
    "    font_path='/home/jovyan/.local/share/fonts/Roboto-Light.ttf'\n",
    ").generate_from_frequencies(fcvdf.sum())\n",
    "plt.imshow(Cloud) \n",
    "plt.axis(\"off\");\n",
    "\n",
    "# Word Clouds for TF/IDF Weighting\n",
    "tfidf.sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "Cloud = WordCloud(\n",
    "    background_color=\"white\", \n",
    "    max_words=100,\n",
    "    font_path='/home/jovyan/.local/share/fonts/JetBrainsMono-VariableFont_wght.ttf'\n",
    ").generate_from_frequencies(tfidf.sum())\n",
    "plt.imshow(Cloud) \n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Dirchlet Allocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Gensim library\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2)) # Notice change to ngram range (try 1,1 and 1,2 for other options)\n",
    "\n",
    "# Calculating the topics\n",
    "vectorizer.fit(corpus) \n",
    "tcorpus = vectorizer.transform(corpus) # tcorpus for transformed corpus\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=3, random_state=42) # Might want to experiment with n_components too\n",
    "LDA.fit(tcorpus)\n",
    "\n",
    "first_topic = LDA.components_[0]\n",
    "top_words = first_topic.argsort()[-25:]\n",
    "\n",
    "for i in top_words:\n",
    "    print(vectorizer.get_feature_names_out()[i])\n",
    "\n",
    "\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    as_markdown(f'Top 10 words for topic #{i}', ', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]]))\n",
    "\n",
    "# Maximize likelihood topic\n",
    "topic_values = LDA.transform(tcorpus)\n",
    "topic_values.shape\n",
    "\n",
    "pd.options.display.max_colwidth=20\n",
    "srcdf['Topic'] = topic_values.argmax(axis=1)\n",
    "srcdf.head()\n",
    "\n",
    "pd.options.display.max_colwidth=75\n",
    "srcdf[srcdf.Topic==1].description_norm.head(10)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', analyzer='word', max_df=0.7, min_df=0.05)\n",
    "topic_corpus = vectorizer.fit_transform(srcdf[srcdf.Topic==1].description.values) # tcorpus for transformed corpus\n",
    "\n",
    "topicdf = pd.DataFrame(data=topic_corpus.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "Cloud = WordCloud(background_color=\"white\", max_words=75).generate_from_frequencies(topicdf.sum())\n",
    "plt.imshow(Cloud) \n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word 2 Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "dims = 100\n",
    "print(f\"You've chosen {dims} dimensions.\")\n",
    "\n",
    "window = 3\n",
    "print(f\"You've chosen a window of size {window}.\")\n",
    "\n",
    "min_v_freq  = 0.005 # Don't keep words appearing less than 0.5% frequency\n",
    "min_v_count = math.ceil(min_v_freq * srcdf.shape[0])\n",
    "print(f\"With a minimum frequency of {min_v_freq} and {srcdf.shape[0]:,} documents, minimum vocab frequency is {min_v_count:,}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "corpus      = srcdf.description_norm.fillna(' ').values\n",
    "#corpus_sent = [nltk.sent_tokenize(text) for text in corpus] # <-- with more formal writing this would work well\n",
    "corpus_sent = [d.replace('.',' ').split(' ') for d in corpus] # <-- deals better with many short sentences though context may end up... weird\n",
    "model       = Word2Vec(sentences=corpus_sent, vector_size=dims, window=window, epochs=200, \n",
    "                 min_count=min_v_count, seed=42, workers=1)\n",
    "\n",
    "model.save(f\"word2vec-d{dims}-w{window}.model\") # <-- You can then Word2Vec.load(...) which is useful with large corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',150)\n",
    "\n",
    "df = fcounts[1] # <-- copy out only the unigrams as we haven't trained anything else\n",
    "\n",
    "n     = 14 # number of words\n",
    "topn  = 7  # number of most similar words\n",
    "\n",
    "selected_words = df[df['Ngram Size 1'] > 5].reset_index().level_0.sample(n, random_state=42).tolist()\n",
    "\n",
    "words = []\n",
    "v1    = []\n",
    "v2    = []\n",
    "v3    = []\n",
    "sims  = []\n",
    "\n",
    "for w in selected_words:\n",
    "    try: \n",
    "        vector = model.wv[w]  # get numpy vector of a word\n",
    "        #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n",
    "    \n",
    "        sim = model.wv.most_similar(w, topn=topn)\n",
    "        #print(f\"Similar words to '{w}' include: {sim}.\")\n",
    "    \n",
    "        words.append(w)\n",
    "        v1.append(vector[0])\n",
    "        v2.append(vector[1])\n",
    "        v3.append(vector[2])\n",
    "        sims.append(\", \".join([x[0] for x in sim]))\n",
    "    except KeyError:\n",
    "        print(f\"Didn't find {w} in model. Can happen with low-frequency terms.\")\n",
    "    \n",
    "vecs = pd.DataFrame({\n",
    "    'Term':words,\n",
    "    'V1':v1, \n",
    "    'V2':v2, \n",
    "    'V3':v3,\n",
    "    f'Top {topn} Similar':sims\n",
    "})\n",
    "\n",
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London?\n",
    "\n",
    "::: duedate\n",
    "( 45 points; Answer due {{< var assess.group-date >}} )\n",
    ":::\n",
    "\n",
    "## Sustainable Authorship Tools\n",
    "\n",
    "Your QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.\n",
    "\n",
    "Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) ([sansfont]{style=\"font-family:Sans-Serif;\"}) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`).\n",
    "\n",
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
